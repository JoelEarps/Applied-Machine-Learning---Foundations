{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance/ Variance Trade off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model optimisation\n",
    "Think of it like a darts model\n",
    "Low bias, Low variance - hitting bulleye and consistently hitting it \n",
    "low bias, high vairance - centred around bullseye - but not contantsly hitting\n",
    "High bias, low variance - not centred around bullseye but darts are not spread\n",
    "High vias, high vairance - not centred around bullseye, and large spread\n",
    "Bias - algortihms tendency to consistently learn the wrong thing by not taking into account all info in data.\n",
    "High bias is a result of the alogrithm missing the relevant relations between features and target outputs. \n",
    "Missing the target, doesn't understand what it should be aiming for soe predictions are systematically off base\n",
    "Vairance - sensitivity to small fluctuations in training data\n",
    "High variance is a result of fitting of random noise in the training data\n",
    "\n",
    "More complexity = higher vairance\n",
    "less complexity = higher bias\n",
    "\n",
    "Finding right model complexity than minimise bias and complexity\n",
    "\n",
    "Total error = bias + variance + irreducable error (some amount of error model will never be able to learn) \n",
    "only control bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underfitting occurs when algorithm cant understand the underlying trends of the data\n",
    "So model is too simple with high bias and low vairance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low bias but high vairance\n",
    "So its just memorised the examples and therefore doesn't make future predictions very well\n",
    "It fits too closely to a limited set of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the optimal trade off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to be in the middle so it gets the overall trend but will miss some data but it has the right decision boundary\n",
    "So for underfitting you will have high training error and high test error\n",
    "Overfitting will have low training error and high test error\n",
    "Whereas an optimal model will have low training and test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways to tune a model:\n",
    "1. Hyperparamter tuning - chosing a set of optimal hyper parameters for fitting an algorithm\n",
    "2. Regularisation - technique used to reduce over fitting by discouraging overly complex models in some way\n",
    "\n",
    "A model parameter is a configuration variable that is internal to model. Whose paramter is estimated from the data\n",
    "A hyper paramter is a configration external to the model, whose value cannot be estimated from data and wholse value guides how the algorithm learns parameter values from data\n",
    "\n",
    "So in a decision tree  paramter are the decision points (and values of the branches i.e. age < 30, 30 is the paramter as the model decided 30 is important).\n",
    "Hyper parameter would be stuff liek depth of the tree, features to consider\n",
    "\n",
    "Grid search - tool in scikitlearn to tune hyper parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technique used to reduce over fitting by discouraging overly complex models in some way\n",
    "Allow enough flexibility to learn underlying patterns in the data but provide guard rails so it doesn't over fit\n",
    "Occams razor - choose the simplest solution to a problem whenever you can\n",
    "Examples of regularisation:\n",
    "1. Ridge regression adn lasso regression - add a penalty to loss function that constraits to constrain coneffecients\n",
    "2. Drop out - some nodes are ignored during training which forces the other nodes to take more or less responsibility with forces other nodes to take more or less responsbility for the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
